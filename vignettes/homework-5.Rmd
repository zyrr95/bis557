---
title: "Homework-5"
author: "Yuren Zhang"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{The Lasso Handwritting Recognization}
-->

#1.
##a. Loading required packages
```{r}
library(keras)
install_keras()
library(keras)
library(glmnet)
#library(doMC)
```

##b.Preparing parallel and set seed
```{r}
#registerDoMC(cores = 4)

set.seed(123)
```

##c. shuffle and split data into training and test

```{r}
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

x_train <- array_reshape(x_train, c(60000, 28^2))
x_test <- array_reshape(x_test, c(10000, 28^2))
y_train <- factor(y_train)
y_test <- factor(y_test)
```

##d. the original accuracy

```{r}
s <- sample(seq_along(y_train), 1000)
fit <- cv.glmnet(x_train[s,], y_train[s], alpha = 1,family = "multinomial",nfolds = 5)
preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min, 
                 type = "class")

t <- table(as.vector(preds), y_test)
sum(diag(t)) / sum(t)
```

##e. extracting features
```{r}
mt = as.matrix(coef(fit))
co = cbind(mt[[1]],mt[[2]],mt[[3]],mt[[4]],mt[[5]],mt[[6]],mt[[7]],mt[[8]],mt[[9]],mt[[10]])
co_m = as.matrix(co)
df = as.data.frame(co_m)[-1,]
rownames(df) = 1:dim(df)[1]
df_r = df[rowSums(df)>0,]

x_train = x_train[,as.numeric(rownames(df_r))]
x_test = x_test[,as.numeric(rownames(df_r))]
```

##f. run new predictions

```{r}
fit <- cv.glmnet(x_train[s,], y_train[s], alpha = 0,family = "multinomial",nfolds = 5)

preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min, 
                 type = "class")


t <- table(as.vector(preds), y_test)
sum(diag(t)) / sum(t)
```

We can see that there is a small increase in the prediction accuracy.

#2. 
Disclaimer: The data used is mnist dataset
Code was Not evaluated on Travis. 


```{r}
x_train = as.matrix(x_train)
x_test = as.matrix(x_test)
y_train = as.matrix(y_train)
y_test = as.matrix(y_test)
```

From the book 8.10.4 we have the original neural net:

```{r,eval= FALSE}
model <- keras_model_sequential()
model %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),
input_shape = c(28, 28, 1),
                padding = "same") %>%
layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),
              padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.5) %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
              padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.5) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")

model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
model %>%
fit(x_train, y_train, epochs = 10,
    validation_data = list(x_test, y_test))
```

Tuning the old model by changeing layer size to (3,3) and dropout rate to 0.25 to get better accuracy:

```{r, eval=FALSE}
model1 <- keras_model_sequential()
model1 %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
input_shape = c(28, 28, 1),
                padding = "same") %>%
layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),
              padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
              padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(2,2),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dropout(rate = 0.25) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")

model1 %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
model1 %>%
fit(x_train, y_train, epochs = 10,
    validation_data = list(x_test, y_test))
```

We see that there is a slight increase of about 1.5%. 

#3

First, generate dataset, from CASL p216 we have
```{r}
X <- matrix(runif(1000, min=-1, max=1), ncol=1)
```

To generate large outliers, we select random locations and add or minus 100, reference:https://stat.ethz.ch/pipermail/r-help/2010-October/254822.html

```{r}
toreplace <- sample(x = seq_along(X), size = 5, replace = FALSE)
X[toreplace] <- runif(n = 5, min = -100, max = 100)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
```

From the textbook, we have the following functions:

```{r}
# Create list of weights to describe a dense neural network.
#
# Args:
#     sizes: A vector giving the size of each layer, including
#            the input and output layers.
#
# Returns:
#     A list containing initialized weights and biases.
#
casl_nn_make_weights <- function(sizes) {
  L <- length(sizes) - 1L
  weights <- vector("list", L)
  for (j in seq_len(L)) {
    w <- matrix(rnorm(sizes[j] * sizes[j + 1L]),
                ncol = sizes[j],
                nrow = sizes[j + 1L])
    weights[[j]] <- list(w=w, b=rnorm(sizes[j + 1L]))
  }
  weights
}
```

```{r}
# Apply a rectified linear unit (ReLU) to a vector/matrix.
#
# Args:
#     v: A numeric vector or matrix.
#
# Returns:
#     The original input with negative values truncated to zero.
#
casl_util_ReLU <- function(v) {
  v[v < 0] <- 0
  v
}
```

```{r}
# Apply derivative of the rectified linear unit (ReLU).
#
# Args:
#     v: A numeric vector or matrix.
#
# Returns:
#     Sets positive values to 1 and negative values to zero.
#
casl_util_ReLU_p <- function(v) {
  p <- v * 0
  p[v > 0] <- 1
  p
}
```

```{r}
# Derivative of the mean absolute deviation (MAD) function.
#
# Args:
#     y: A numeric vector of responses.
#     a: A numeric vector of predicted responses.
#
# Returns:
#     Returned current derivative the MAD function.
# 
casl_util_mse_p <- function(y, a){
  mad <- NULL
  for(i in 1:length(a)){
   mad = ifelse(a[i] >= y[i],1,-1)
  }
  mad
}
```

```{r}
# Apply forward propagation to a set of NN weights and biases.
#
# Args:
#     x: A numeric vector representing one row of the input.
#     weights: A list created by casl_nn_make_weights.
#     sigma: The activation function.
#
# Returns:
#     A list containing the new weighted responses (z) and
#     activations (a).
#
casl_nn_forward_prop <- function(x, weights, sigma) {
  L <- length(weights)
  z <- vector("list", L)
  a <- vector("list", L)
  for (j in seq_len(L)) {
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    z[[j]] <- weights[[j]]$w %*% a_j1 + weights[[j]]$b
    a[[j]] <- if (j != L) sigma(z[[j]]) else z[[j]]
  }
  list(z=z, a=a)
}
```

```{r}
# Apply backward propagation algorithm.
#
# Args:
#     x: A numeric vector representing one row of the input.
#     y: A numeric vector representing one row of the response.
#     weights: A list created by casl_nn_make_weights.
#     f_obj: Output of the function casl_nn_forward_prop.
#     sigma_p: Derivative of the activation function.
#     f_p: Derivative of the loss function.
#
# Returns:
#     A list containing the new weighted responses (z) and
#     activations (a).
#
casl_nn_backward_prop <- function(x, y, weights, f_obj, sigma_p, f_p) {
  z <- f_obj$z
  a <- f_obj$a
  L <- length(weights)
  grad_z <- vector("list", L)
  grad_w <- vector("list", L)
  for (j in rev(seq_len(L))) {
    if (j == L) {
      grad_z[[j]] <- f_p(y, a[[j]])
    } 
    else {
      grad_z[[j]] <- (t(weights[[j + 1]]$w) %*% grad_z[[j + 1]]) * sigma_p(z[[j]])
    }
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    grad_w[[j]] <- grad_z[[j]] %*% t(a_j1)
  }
  list(grad_z=grad_z, grad_w=grad_w)
}
```

```{r}
# Apply stochastic gradient descent (SGD) to estimate NN.
#
# Args:
#     X: A numeric data matrix.
#     y: A numeric vector of responses.
#     sizes: A numeric vector giving the sizes of layers in
#     the neural network.
#     epochs: Integer number of epochs to computer.
#     eta: Positive numeric learning rate.
#     weights: Optional list of starting weights.
#
# Returns:
#     A list containing the trained weights for the network.
#
casl_nn_sgd <- function(X, y, sizes, epochs, eta, weights=NULL) {
  if (is.null(weights)) {
    weights <- casl_nn_make_weights(sizes)
  }
  for (epoch in seq_len(epochs)) {
    for (i in seq_len(nrow(X))) {
      f_obj <- casl_nn_forward_prop(X[i,], weights, casl_util_ReLU)
      b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
                                     f_obj, casl_util_ReLU_p,
                                     casl_util_mse_p)
      for (j in seq_along(b_obj)) {
        weights[[j]]$b <- weights[[j]]$b - eta * b_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w - eta * b_obj$grad_w[[j]]
      }
    }
  }
  weights
}
```

```{r}
# Predict values from a training neural network.
#
# Args:
#     weights: List of weights describing the neural network.
#     X_test: A numeric data matrix for the predictions.
#
# Returns:
#     A matrix of predicted values.
# 
casl_nn_predict <- function(weights, X_test) {
  p <- length(weights[[length(weights)]]$b)
  y_hat <- matrix(0, ncol = p, nrow = nrow(X_test))
  for (i in seq_len(nrow(X_test))) {
    a <- casl_nn_forward_prop(X_test[i,], weights, casl_util_ReLU)$a
    y_hat[i,] <- a[[length(a)]]
  }
  y_hat
}
```




